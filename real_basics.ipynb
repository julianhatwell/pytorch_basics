{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch # pytorch library\n",
    "from torch.autograd import Variable # variables may accumulate gradients\n",
    "import torchvision.transforms as transforms\n",
    "from torch.distributions import normal\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare numpy and torch arrays/tensors\n",
    "# numpy\n",
    "array = [[1,2,3],[4,5,6]]\n",
    "first_array = np.array(array) # 2x3 array\n",
    "print(\"Array Type: {}\".format(type(first_array))) # type\n",
    "print(\"Array Shape: {}\".format(np.shape(first_array))) # shape\n",
    "print(first_array)\n",
    "print()\n",
    "\n",
    "# torch\n",
    "tensor = torch.Tensor(array) # from python list\n",
    "print(\"Array Type: {}\".format(tensor.type)) # type\n",
    "print(\"Array Shape: {}\".format(tensor.shape)) # shape\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIALISATION\n",
    "\n",
    "# numpy ones\n",
    "print(\"Numpy\\n {}\\n\".format(np.ones((2,3))))\n",
    "\n",
    "# pytorch ones\n",
    "print(torch.ones((2,3)))\n",
    "print()\n",
    "\n",
    "# numpy random\n",
    "print(\"Numpy\\n {}\\n\".format(np.random.rand(5,3)))\n",
    "\n",
    "# pytorch random\n",
    "print(torch.rand(5,3))\n",
    "print()\n",
    "\n",
    "# uninitialised - whatever values were in allocated memory space\n",
    "x = torch.empty(5, 3)\n",
    "print(x)\n",
    "print()\n",
    "\n",
    "# initialise with zeroes\n",
    "x = torch.zeros(5, 3, dtype=torch.long)\n",
    "print(x)\n",
    "print()\n",
    "\n",
    "# initialise with values\n",
    "x = torch.tensor([5.5, 3])\n",
    "print(x)\n",
    "print()\n",
    "\n",
    "# set new values\n",
    "x = x.new_ones(5, 3, dtype=torch.double)      # new_* methods take in sizes\n",
    "print(x)\n",
    "\n",
    "# set new values\n",
    "x = torch.randn_like(x, dtype=torch.float)    # override dtype!\n",
    "print(x)\n",
    "print()\n",
    "\n",
    "print(x.size())\n",
    "# torch.Size supports tuple operations\n",
    "print()\n",
    "\n",
    "# numpy style indexing\n",
    "print(x[:, 1])\n",
    "\n",
    "# reshaping\n",
    "x = torch.randn(4, 4)\n",
    "y = x.view(16)\n",
    "z = x.view(-1, 8)  # the size -1 is inferred from other dimensions\n",
    "print(x.size(), y.size(), z.size())\n",
    "\n",
    "# extract a scalar value\n",
    "x = torch.randn(1)\n",
    "print(x)\n",
    "print(x.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random numpy array\n",
    "array = np.random.rand(2,2)\n",
    "print(\"{}\\n {}\\n\".format(type(array),array))\n",
    "\n",
    "# from numpy to tensor\n",
    "from_numpy_to_tensor = torch.from_numpy(array)\n",
    "print(\"{}\\n\".format(from_numpy_to_tensor))\n",
    "\n",
    "# from tensor to numpy\n",
    "from_tensor_to_numpy = from_numpy_to_tensor.numpy()\n",
    "print(\"{}\\n {}\\n\".format(type(from_tensor_to_numpy),from_tensor_to_numpy))\n",
    "print()\n",
    "\n",
    "# bridge to numpy - note that memory is shared\n",
    "a = torch.ones(5)\n",
    "print(a)\n",
    "b = a.numpy()\n",
    "print(b)\n",
    "a.add_(1)\n",
    "print(a)\n",
    "print(b)\n",
    "\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a, 1, out=a)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tensor \n",
    "tensor = torch.ones(3,3)\n",
    "print(\"\\n \",tensor)\n",
    "\n",
    "# Resize with view\n",
    "print(\"Resise:\\n {} {} \\n\".format(tensor.view(9).shape, tensor.view(9)))\n",
    "\n",
    "# Addition\n",
    "print(\"Addition:\\n {}\\n\".format(torch.add(tensor,tensor)))\n",
    "print(\"Addition:\\n {}\\n\".format(tensor.add(tensor)))\n",
    "print(\"Addition:\\n {}\\n\".format(tensor + tensor))\n",
    "\n",
    "print('\\nalso adding in place')\n",
    "tensor.add_(tensor)\n",
    "print(tensor)\n",
    "tensor.add_(tensor)\n",
    "print(tensor)\n",
    "print()\n",
    "\n",
    "# Subtraction\n",
    "print(\"Subtraction:\\n {}\\n\".format(torch.sub(tensor,tensor)))\n",
    "print(\"Subtraction:\\n {}\\n\".format(tensor.sub(tensor)))\n",
    "print(\"Subtraction:\\n {}\\n\".format(tensor - tensor))\n",
    "\n",
    "# Element wise multiplication\n",
    "print(\"Element wise multiplication:\\n {}\\n\".format(torch.mul(tensor + tensor, tensor + tensor)))\n",
    "print(\"Element wise multiplication:\\n {}\\n\".format(tensor.mul(tensor + tensor)))\n",
    "print(\"Element wise multiplication:\\n {}\\n\".format((tensor + tensor) * (tensor + tensor)))\n",
    "\n",
    "# Element wise division\n",
    "print(\"Element wise division:\\n {}\\n\".format(torch.div(tensor,tensor)))\n",
    "print(\"Element wise division:\\n {}\\n\".format(tensor.div(tensor)))\n",
    "print(\"Element wise division:\\n {}\\n\".format(tensor / (tensor + tensor)))\n",
    "\n",
    "# Mean\n",
    "tensor = torch.Tensor([[1,2,3,4,5], [6,7,8,9, 10]])\n",
    "print(\"Mean:\\n {}\".format(tensor.mean(axis=1)))\n",
    "\n",
    "# Standard deviation (std)\n",
    "print(\"std:\\n {}\".format(tensor.std(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define variable - these are like tensors but optionally keep track of gradients\n",
    "var = Variable(torch.ones(3), requires_grad = True)\n",
    "var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick look at backward propagation\n",
    "input_values = torch.Tensor([2,3,4,5])\n",
    "x = Variable(input_values, requires_grad = True)\n",
    "print(\" x =  \",x)\n",
    "# perform some transformation on x\n",
    "y = x**2\n",
    "print(\" y =  \",y)\n",
    "\n",
    "# and a further calculation, e.g. a loss/cost function\n",
    "o = sum((y - input_values)**2)/len(input_values)\n",
    "print(\" o =  \",o)\n",
    "\n",
    "# backward\n",
    "o.backward() # calculates gradients, which go back to x\n",
    "print(\"x gradients: \", x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear regression example\n",
    "\n",
    "# sales regress on prices\n",
    "n = 7\n",
    "price_tensor = torch.Tensor([i+3 for i in range(n)]).view(-1,1)\n",
    "price_tensor\n",
    "\n",
    "# some random error (this sampling method creates a tensor of shape (1, n))\n",
    "err = normal.Normal(torch.tensor([0.0]), torch.tensor([0.5]))\n",
    "sales_tensor = torch.add(torch.Tensor([i+.5 for i in range(n,0,-1)]).view(-1,1),\n",
    "                         err.sample(torch.tensor([n])))\n",
    "\n",
    "# here's our linear relationship\n",
    "plt.scatter(price_tensor,sales_tensor)\n",
    "plt.xlabel(\"Price ($)\")\n",
    "plt.ylabel(\"Sales (1000's)\")\n",
    "plt.title(\"Price vs Sales\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create class - must have a forward function\n",
    "# inherit everything from nn.Module\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self,input_size,output_size):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        # Built in linear function\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "    # all models need a forward function defining the model structure\n",
    "    # this one is simple\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "# define model\n",
    "input_dim = 1\n",
    "output_dim = 1\n",
    "model = LinearRegression(input_dim, output_dim) # input and output size are 1\n",
    "\n",
    "# MSE loss\n",
    "mse = nn.MSELoss()\n",
    "\n",
    "# Optimization\n",
    "learning_rate = 0.02\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "n_iter = 1000\n",
    "iloss = np.empty(n_iter)\n",
    "for i in range(n_iter):\n",
    "        \n",
    "    # reset the gradients\n",
    "    optimizer.zero_grad() \n",
    "    \n",
    "    # Forward to get output - is there some kind of aliasing? forward is not explicitly called\n",
    "    results = model(price_tensor)\n",
    "    \n",
    "    # Calculate Loss\n",
    "    loss = mse(results, sales_tensor)\n",
    "    \n",
    "    # backward propagation - calculates gradients back through all the model parameters\n",
    "    loss.backward()\n",
    "    \n",
    "    # Updating parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    # info to UI\n",
    "    iloss[i] = loss.data\n",
    "    if(i % 50 == 0):\n",
    "        print('epoch {}, loss {}'.format(i, loss.data))\n",
    "\n",
    "plt.plot(range(n_iter),iloss)\n",
    "plt.xlabel(\"Number of Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict our car price \n",
    "predicted = model(price_tensor)\n",
    "plt.scatter(price_tensor,sales_tensor,\n",
    "            label = \"original data\",color =\"red\")\n",
    "plt.scatter(price_tensor,model(price_tensor).data,\n",
    "            label = \"predicted data\",color =\"blue\")\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"Car Price ($)\")\n",
    "plt.ylabel(\"Sales (1000's)\")\n",
    "plt.title(\"Original vs Predicted\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# executing in the GPU\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "# We will use ``torch.device`` objects to move tensors in and out of GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # a CUDA device object\n",
    "    y = torch.ones_like(x, device=device)  # directly create a tensor on GPU\n",
    "    x = x.to(device)                       # or just use strings ``.to(\"cuda\")``\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
