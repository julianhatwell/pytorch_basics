{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable # variables may accumulate gradients\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"./data/kaggle1\"))\n",
    "\n",
    "# generic training function\n",
    "def train_(model, resize, #  e.g. lambda x : x.view(-1, 28)\n",
    "            optimizer, error,\n",
    "            train_loader, test_loader,\n",
    "            batch_size, n_iters, n_epochs):\n",
    "    \n",
    "    count = 0\n",
    "    loss_list = []\n",
    "    iteration_list = []\n",
    "    accuracy_list = []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "\n",
    "            # Define variables\n",
    "            train = resize(images) # create a column tensor\n",
    "\n",
    "            # Clear gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward propagation\n",
    "            outputs = model(train)\n",
    "\n",
    "            # Calculate softmax and cross entropy loss\n",
    "            loss = error(outputs, labels)\n",
    "\n",
    "            # Calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            count += 1\n",
    "\n",
    "            # Prediction\n",
    "            if count % 50 == 0:\n",
    "                # Calculate Accuracy         \n",
    "                correct = 0\n",
    "                total = 0\n",
    "                # Predict test dataset\n",
    "                for images, labels in test_loader: \n",
    "                    test = resize(images)\n",
    "\n",
    "                    # Forward propagation\n",
    "                    outputs = model(test)\n",
    "\n",
    "                    # Get predictions from the maximum value\n",
    "                    predicted = torch.max(outputs.data, 1)[1]\n",
    "\n",
    "                    # Total number of labels\n",
    "                    total += len(labels)\n",
    "\n",
    "                    # Total correct predictions\n",
    "                    correct += (predicted == labels).sum()\n",
    "\n",
    "                accuracy = 100 * correct / float(total)\n",
    "                loss_list.append(loss.data)\n",
    "                iteration_list.append(count)\n",
    "                accuracy_list.append(accuracy)\n",
    "\n",
    "                if count % 500 == 0:\n",
    "                    # Print Loss\n",
    "                    print('Epoch: {}  Iteration: {}  Loss: {}  Accuracy: {}%'.format(epoch,\n",
    "                                                                                     count,\n",
    "                                                                                     loss.data,\n",
    "                                                                                     accuracy))\n",
    "    return(model, {'loss_list' : loss_list,\n",
    "                   'accuracy_list' : accuracy_list,\n",
    "                   'iteration_list' : iteration_list})\n",
    "\n",
    "# generic plot func\n",
    "def measures_plot(measures):\n",
    "    plt.plot(measures['iteration_list'], measures['loss_list'])\n",
    "    plt.xlabel(\"Number of iteration\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Loss\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(measures['iteration_list'], measures['accuracy_list'], color = \"red\")\n",
    "    plt.xlabel(\"Number of iteration\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Accuracy\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hand-written digit classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train = pd.read_csv(r\"./data/kaggle1/train.csv\", dtype = np.float32)\n",
    "\n",
    "# split data into features(pixels) and labels(numbers from 0 to 9)\n",
    "targets_numpy = train.label.values\n",
    "features_numpy = train.loc[:,train.columns != \"label\"].values/255 # normalization\n",
    "\n",
    "# train test split. Size of train data is 80% and size of test data is 20%. \n",
    "features_train, features_test, targets_train, targets_test = train_test_split(features_numpy,\n",
    "                                                                             targets_numpy,\n",
    "                                                                             test_size = 0.2,\n",
    "                                                                             random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create feature and targets tensor for train set. As you remember we need variable to accumulate gradients. Therefore first we create tensor, then we will create variable\n",
    "featuresTrain = torch.from_numpy(features_train)\n",
    "targetsTrain = torch.from_numpy(targets_train).type(torch.LongTensor) # data type is Long\n",
    "# create feature and targets tensor for test set.\n",
    "featuresTest = torch.from_numpy(features_test)\n",
    "targetsTest = torch.from_numpy(targets_test).type(torch.LongTensor) # data type is Long\n",
    "\n",
    "# Pytorch train and test sets\n",
    "train_set = torch.utils.data.TensorDataset(featuresTrain,targetsTrain)\n",
    "test_set = torch.utils.data.TensorDataset(featuresTest,targetsTest)\n",
    "\n",
    "height = 28\n",
    "width = 28\n",
    "n_pixels = height*width\n",
    "n_classes = 10 # labels 0,1,2,3,4,5,6,7,8,9\n",
    "\n",
    "# visualize one of the images in data set\n",
    "plt.imshow(featuresTrain[10].reshape(height, width))\n",
    "plt.axis(\"off\")\n",
    "plt.title(str(targetsTrain[10]))\n",
    "plt.savefig('graph.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Logistic Regression Model\n",
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        # Linear part\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        # Logistic function in pytorch is in loss function - comes later\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "\n",
    "# Instantiate Model\n",
    "model = LogisticRegressionModel(input_dim=n_pixels, output_dim=n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_iters = 10000\n",
    "\n",
    "# data loader\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "# Cross Entropy Loss  \n",
    "error = nn.CrossEntropyLoss()\n",
    "\n",
    "# SGD Optimizer \n",
    "learning_rate = 0.02\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "model, measures = train_(model, resize = lambda x : x.view(-1, n_pixels), # this will convert each instance to a column vec\n",
    "                         optimizer = optimizer, error = error,\n",
    "                         train_loader = train_loader,\n",
    "                         test_loader = test_loader,\n",
    "                         batch_size = batch_size,\n",
    "                         n_iters = n_iters,\n",
    "                         n_epochs = int(n_iters / (len(features_train) / batch_size)))\n",
    "\n",
    "measures_plot(measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ANN Model\n",
    "class ANNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(ANNModel, self).__init__()\n",
    "        # Linear function 1: 784 --> 100\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim) \n",
    "        # Non-linearity 1\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        # Linear function 2: 100 --> 100\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        # Non-linearity 2\n",
    "        self.tanh2 = nn.Tanh()\n",
    "        \n",
    "        # Linear function 3: 100 --> 100\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        # Non-linearity 3\n",
    "        self.elu3 = nn.ELU()\n",
    "        \n",
    "        # Linear function 4 (readout): 100 --> 10\n",
    "        self.fc4 = nn.Linear(hidden_dim, output_dim)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Linear function 1\n",
    "        out = self.fc1(x)\n",
    "        # Non-linearity 1\n",
    "        out = self.relu1(out)\n",
    "        \n",
    "        # Linear function 2\n",
    "        out = self.fc2(out)\n",
    "        # Non-linearity 2\n",
    "        out = self.tanh2(out)\n",
    "        \n",
    "        # Linear function 2\n",
    "        out = self.fc3(out)\n",
    "        # Non-linearity 2\n",
    "        out = self.elu3(out)\n",
    "        \n",
    "        # Linear function 4 (readout)\n",
    "        out = self.fc4(out)\n",
    "        return out\n",
    "\n",
    "# instantiate ANN\n",
    "input_dim = n_pixels\n",
    "hidden_dim = 100 # to be tuned as a hyper-param\n",
    "output_dim = n_classes\n",
    "\n",
    "# Create ANN\n",
    "model = ANNModel(input_dim, hidden_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_iters = 10000\n",
    "\n",
    "# data loader\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "# Cross Entropy Loss  \n",
    "error = nn.CrossEntropyLoss()\n",
    "\n",
    "# SGD Optimizer \n",
    "learning_rate = 0.02\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "model, measures = train_(model, resize = lambda x : x.view(-1, n_pixels), # this will convert each instance to a column vec\n",
    "                         optimizer = optimizer, error = error,\n",
    "                         train_loader = train_loader,\n",
    "                         test_loader = test_loader,\n",
    "                         batch_size = batch_size,\n",
    "                         n_iters = n_iters,\n",
    "                         n_epochs = int(n_iters / (len(features_train) / batch_size)))\n",
    "\n",
    "measures_plot(measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using nn.Sequential\n",
    "hidden1_dim = 60\n",
    "hidden2_dim = 40\n",
    "hidden3_dim = 20\n",
    "\n",
    "def add_layer(input_dim, output_dim, activation='relu'):\n",
    "    activations = nn.ModuleDict([\n",
    "        ['sig', nn.Sigmoid()],\n",
    "        ['tanh', nn.Tanh()],\n",
    "        ['lrelu', nn.LeakyReLU()],\n",
    "        ['relu', nn.ReLU()],\n",
    "        ['elu', nn.ELU()]\n",
    "    ])\n",
    "    \n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, output_dim),\n",
    "        activations[activation]\n",
    "    )\n",
    "\n",
    "model = nn.Sequential(\n",
    "    add_layer(n_pixels, hidden1_dim, 'relu'),\n",
    "    add_layer(hidden1_dim, hidden2_dim, 'tanh'),\n",
    "    add_layer(hidden2_dim, hidden3_dim, 'elu'),\n",
    "    nn.Linear(hidden3_dim, n_classes)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_iters = 10000\n",
    "\n",
    "# data loader\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "# Cross Entropy Loss  \n",
    "error = nn.CrossEntropyLoss()\n",
    "\n",
    "# SGD Optimizer \n",
    "learning_rate = 0.02\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "model, measures = train_(model, resize = lambda x : x.view(-1, n_pixels), # this will convert each instance to a column vec\n",
    "                         optimizer = optimizer, error = error,\n",
    "                         train_loader = train_loader,\n",
    "                         test_loader = test_loader,\n",
    "                         batch_size = batch_size,\n",
    "                         n_iters = n_iters,\n",
    "                         n_epochs = int(n_iters / (len(features_train) / batch_size)))\n",
    "\n",
    "measures_plot(measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpk = 2\n",
    "\n",
    "def conv_block(in_f, out_f, activation, *args, **kwargs):\n",
    "    activations = nn.ModuleDict([\n",
    "        ['sig', nn.Sigmoid()],\n",
    "        ['tanh', nn.Tanh()],\n",
    "        ['lrelu', nn.LeakyReLU()],\n",
    "        ['relu', nn.ReLU()],\n",
    "        ['elu', nn.ELU()]\n",
    "    ])\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_f, out_f, *args, **kwargs),\n",
    "        activations[activation],\n",
    "        nn.MaxPool2d(kernel_size=mpk)\n",
    "    )\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, in_c, enc_sizes, activations, n_classes):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.enc_sizes = [in_c, *enc_sizes]\n",
    "        \n",
    "        #         self.encoder = nn.Sequential(\n",
    "        #             conv_block(1, 16, kernel_size=5, stride=1, padding=0),\n",
    "        #             conv_block(16, 32, kernel_size=5, stride=1, padding=0)\n",
    "        #         )\n",
    "        conv_blocks = [conv_block(in_f, out_f, act, kernel_size=5, stride=1, padding=0) \n",
    "                       for in_f, out_f, act in zip(self.enc_sizes, self.enc_sizes[1:], activations)]\n",
    "        \n",
    "        self.encoder = nn.Sequential(*conv_blocks)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            # * 4 * 4 is from two lots of max pooling kernel size 2\n",
    "            # base value should match last channel size\n",
    "            nn.Linear(enc_sizes[len(enc_sizes)-1] * 4 * 4, n_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.encoder(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.decoder(out)\n",
    "        return(out)\n",
    "\n",
    "# Create ANN\n",
    "model = CNN(in_c=1, enc_sizes=[16, 32], activations=['relu', 'elu'], n_classes=n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_iters = 2500\n",
    "\n",
    "# data loader\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "# Cross Entropy Loss  \n",
    "error = nn.CrossEntropyLoss()\n",
    "\n",
    "# SGD Optimizer \n",
    "learning_rate = 0.02\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "model, measures = train_(model, resize = lambda x : x.view(-1,1,28,28), # this will convert each instance to a column vec\n",
    "                         optimizer = optimizer, error = error,\n",
    "                         train_loader = train_loader,\n",
    "                         test_loader = test_loader,\n",
    "                         batch_size = batch_size,\n",
    "                         n_iters = n_iters,\n",
    "                         n_epochs = int(n_iters / (len(features_train) / batch_size)))\n",
    "\n",
    "measures_plot(measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RNN Model\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super(RNNModel, self).__init__()\n",
    "        # Number of hidden dimensions\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Number of hidden layers\n",
    "        self.layer_dim = layer_dim\n",
    "        \n",
    "        # RNN\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, layer_dim, batch_first=True, \n",
    "                          nonlinearity='relu')\n",
    "        \n",
    "        # Readout layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = Variable(torch.zeros(self.layer_dim, x.size(0), self.hidden_dim))\n",
    "            \n",
    "        # One time step\n",
    "        out, hn = self.rnn(x, h0)\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        return out\n",
    "\n",
    "# Create RNN\n",
    "input_dim = 28    # input dimension - use the pixel width for square images\n",
    "hidden_dim = 100  # hidden layer dimension\n",
    "layer_dim = 3   # number of hidden layers\n",
    "\n",
    "model = RNNModel(input_dim, hidden_dim, layer_dim, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_iters = 4500\n",
    "\n",
    "# data loader\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "# Cross Entropy Loss \n",
    "error = nn.CrossEntropyLoss()\n",
    "\n",
    "# SGD Optimizer \n",
    "learning_rate = 0.05\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "model, measures = train_(model, resize = lambda x : x.view(-1, width, width), \n",
    "                         optimizer = optimizer, error = error,\n",
    "                         train_loader = train_loader,\n",
    "                         test_loader = test_loader,\n",
    "                         batch_size = batch_size,\n",
    "                         n_iters = n_iters,\n",
    "                         n_epochs = int(n_iters / (len(features_train) / batch_size)))\n",
    "\n",
    "measures_plot(measures)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
